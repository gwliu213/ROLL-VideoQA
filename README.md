# Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions

This is the PyTorch implementation of our ROLL model. 

![roll](https://github.com/noagarcia/ROLL-VideoQA/blob/master/Images/model.png?raw=true)

ROLL consists on three branches, each performing a different inspired-cognitive task:
1) **Read branch**: Dialog comprehension.
2) **Observe branch**: Visual scene reasoning.
3) **Recall branch**: Storyline recalling. 

The information generated by each branch is encoded via Transformers. A modality weighting mechanism balances the output from the different modalities to predict the final answer.

## Environment Setup

### Dependencies

This code runs on Python 3.6 and PyTorch 0.4. We recommend using [Anaconda](https://www.anaconda.com/) to install the dependencies.
```
conda create --name roll-videoqa python=3.6
conda activate roll-videoqa
conda install -c anaconda numpy pandas scikit-learn 
conda install -c conda-forge visdom tqdm
conda install pytorch==1.0.1 torchvision==0.2.2 -c pytorch
pip install pytorch-transformers
```
List of dependencies:
- [NumPy](https://numpy.org/)
- [pandas](https://pandas.pydata.org/)
- [PyTorch](https://pytorch.org/) 1.0.1 with [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
- [PyTorch-Transformers](https://pypi.org/project/pytorch-transformers/)
- [scikit-learn](https://scikit-learn.org/)
- [tqdm](https://github.com/tqdm/tqdm) for progress bar.
- [Visdom](https://github.com/facebookresearch/visdom) for data visualization.

Optional:
- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to download episode summaries (not necessary, as already provided).




### Data

ROLL is designed to leverage external information to answer knowledge-based questions about videos. 
We reported experiments on two datasets: [KnowIT VQA](https://knowit-vqa.github.io/) and the [TVQA+](http://tvqa.cs.unc.edu/download_tvqa_plus.html). 
Both datasets contain videos from the Big Bang Theory, so list of characters and common locations are shared.


## ROLL on KnowIT VQA
#### Data
1. Download annotations from [here](https://knowit-vqa.github.io/) and extract the zip file contents into `Data/` directory. 
You should get 3 csv files inside `Data/knowit_data/`.
2. The episode summaries used as source of external knowledge in the paper are placed in `Data/KnowledgeBase/tbbt_summaries.csv`. 
The script used to download the summaries can be found in `Tools/download_summaries.py`.
3. Video story identification has been already pre-computed and can be found in `Data/KnowledgeBase/`.
The script used to identify each episode can be found in `Tools/video_story_identification.py`.


#### Training
To see the visualizations during training, first start Visdom in a terminal. Visualizations can be accessed at `http://localhost:8097`.
``` 
python -m visdom.server
```

To train ROLL on KnowIT VQA datset run:

```
bash train.sh knowit
```
The training is performed in two stages: 
1) First, all the three branches (read, observe, recall) are pretrained.
2) Then, the network that fuses the outputs from the branches is trained using the modality weighting mechanism.

#### Inference


## ROLL on TVQA+
TODO.


## TODO
- [X] Read branch
- [ ] Observe branch
- [ ] Recall branch
- [ ] Fusion
- [ ] TVQA+ code
- [ ] Results table
- [ ] Examples
- [ ] Citation

