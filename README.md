# Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions

This is the PyTorch implementation of our ROLL model. 

![roll](https://github.com/noagarcia/ROLL-VideoQA/blob/master/Images/model.png?raw=true)

ROLL consists on three branches, each performing a different inspired-cognitive task:
1) **Read branch**: Dialog comprehension.
2) **Observe branch**: Visual scene reasoning.
3) **Recall branch**: Storyline recalling. 

The information generated by each branch is encoded via Transformers. A modality weighting mechanism balances the output from the different modalities to predict the final answer.

## Environment Setup

### Dependencies

This code runs on Python 3.6 and PyTorch 0.4. We recommend using [Anaconda](https://www.anaconda.com/) to install the dependencies.
```
conda create --name roll-videoqa python=3.6
conda activate roll-videoqa
conda install -c anaconda numpy pandas 
conda install -c conda-forge visdom tqdm
conda install pytorch==1.0.1 torchvision==0.2.2 -c pytorch
pip install pytorch-transformers
```
List of dependencies:
- [NumPy](https://numpy.org/)
- [pandas](https://pandas.pydata.org/)
- [PyTorch](https://pytorch.org/) 1.0.1 with [torchvision](https://pytorch.org/docs/stable/torchvision/index.html).
- [PyTorch-Transformers](https://pypi.org/project/pytorch-transformers/)
- [tqdm](https://github.com/tqdm/tqdm) for progress bar.
- [Visdom](https://github.com/facebookresearch/visdom) for data visualization.



### Data

ROLL is designed to leverage external information to answer knowledge-based questions about videos. 
We reported experiments on two datasets: [KnowIT VQA](https://knowit-vqa.github.io/) and the [TVQA+](http://tvqa.cs.unc.edu/download_tvqa_plus.html). 
Both datasets contain videos from the Big Bang Theory, so list of characters and common locations are shared.


## ROLL on KnowIT VQA
#### Data
Download annotations from [here](https://knowit-vqa.github.io/) and extract the zip file contents into `Data/` directory. You should get 3 csv files inside `Data/knowit_data/`.

#### Training
If you want to see the visualizations during training, first start the Visdom server in a terminal. Visualizations can be accessed at `http://localhost:8097`.
``` 
python -m visdom.server
```

To train ROLL on KnowIT VQA datset run:

```
bash train.sh knowit
```
The training is performed in two stages: 
1) First, all the three branches (read, observe, recall) are pre-trained.
2) Then, the output for each branch are fused and trained together.

#### Inference


## ROLL on TVQA+
TODO.


## TODO
- [X] Read branch
- [ ] Observe branch
- [ ] Recall branch
- [ ] Fusion
- [ ] TVQA+ code
- [ ] Results table
- [ ] Examples
- [ ] Citation

