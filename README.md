# Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions

This is the PyTorch implementation of our ROLL model. 

![roll](https://github.com/noagarcia/ROLL-VideoQA/blob/master/Images/model.png?raw=true)

ROLL consists on three branches, each performing a different inspired-cognitive task:
1) **Read branch**: Dialog comprehension.
2) **Observe branch**: Visual scene reasoning.
3) **Recall branch**: Storyline recalling. 

The information generated by each branch is encoded via Transformers. A modality weighting mechanism balances the output from the different modalities to predict the final answer.


### Dependencies

This code runs on Python 3.6 and PyTorch 0.4. We recommend using [Anaconda](https://www.anaconda.com/) to install the dependencies:
```
conda create --name roll-videoqa python=3.6
conda activate roll-videoqa
```

### Data

ROLL is designed to leverage external information to answer knowledge-based questions about videos. 
We reported experiments on two datasets: [KnowIT VQA](https://knowit-vqa.github.io/) and the [TVQA+](http://tvqa.cs.unc.edu/download_tvqa_plus.html). 
Both datasets contain videos from the Big Bang Theory, so list of characters and common locations are shared.


## KnowIT VQA
TODO.

### TODO
- [ ] List of dependencies
- [ ] Read branch
- [ ] Observe branch
- [ ] Recall branch
- [ ] Fusion
- [ ] TVQA+ code
- [ ] Results table
- [ ] Examples
- [ ] Citation

